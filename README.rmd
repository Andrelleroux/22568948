---
output:
  md_document:
    variant: markdown_github
---

# 22568948 Data Science Practical Exam

In this markdown folder I will explain my thinking and working for the coding aspect of the Data Science Methods for Economics and Finance final exam in June 2024. I will work through the questions one by one and talk through the code that I wrote to complete it.


```{r, results='hide', message=FALSE, warning=FALSE}

rm(list = ls()) # Clean your environment:
gc()
setwd("C:/Users/andre/OneDrive/Documents/Masters_2024_stuff/Data_Science/Data_Science_Exam/22568948")

library(tidyverse)
```

##Question 1

It is firstly important that the working directory of the project is properly allocated as the directory is used to create folders and projects. All Texevier lines of code was commented as the projects had already been created.

```{r, results='hide', message=FALSE, warning=FALSE}
#Firstly I create the Texevier project for writing the report and coding in Question 1
#Texevier::create_template_html(directory = glue::glue("{getwd()}/"), template_name = "Question_1")

#Secondly I fetch the functions that I wrote from the code folder in Question 1
list.files('Question_1/code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))

```

The first command that is run is to create a Texevier HTML project under the title Question_1 wherein I complete the question. The second is to fetch the functions that I create while writing the report for question 1.

```{r, results='hide', message=FALSE, warning=FALSE}
Baby_names_df <- Data_Reading("Question_1/data/Baby_Names_By_US_State.rds")
```

This function simply read the data from the given rds file into a dataframe that can be used in the report. No data transformations take place in this function.

```{r, message=FALSE, warning=FALSE}
plot_corr <- Corr_plot(Baby_names_df)
plot_corr
```

This function transforms the base dataframe to a format that can be used to plot the Spearman correlation over time. The first step is to rank the most popular names from each year across the United States. This is done by grouping by year and name and them summarising to get to get the totals. Then we arrange the totals in descending order according to the totals of each name in each year. We can then rank by group to get the ranking in each year.

Once we have this we can rename the rank variable and mutate the year to be _no_Years_ years earlier (where _no_Years_ can be set, default is 3). We can then left_join the dataframe with the renamed and mutated dataframe to get the rank of the names in the start year and the later year. We then arrange by the rank in the start year and use "slice_head" to get the top _no_top_ names per year (where _no_top_ can be set, the default is 25). Correlation is then calculated between the ranks of the names in the start year and their rank x years later, the Spearman method is used.

This is then plotted using a geometric line graph to show the changes in correlation over time, geometric points are added to highlight the exact points in time and finally a geom_smooth plot is used to show a smoothed mean across time with a confidence interval of _conf_level_ (where _conf_level_ can be set, the default is 0.95).

```{r, message=FALSE, warning=FALSE}
plot_ridge <- Ridge_plot(Baby_names_df)
plot_ridge
```

This first step in this function is to take the same base dataframe and group by name and summarise by count to get the total count of babies per baby name. Using arrange and head we can get the top _no_names_ most used names (where _no_names_ can be set, default is 15). 

Then I can group by Year and Name to summarise by count and filter for the top _no_names_ names in each year. I then use the ggridges package to plot a density ridge plot for the popularity of the most popular names over time. Using alpha = 0.5 to make the graphs more see through. 

##Question 2

This section explains the coding behind question 2 of the exam. The following code was used to create the project and to fetch the function to be used in Question 2 from the code folder.

```{r, results='hide', message=FALSE, warning=FALSE}
#Firstly Create the Texevier project
#Texevier::create_template_html(directory = glue::glue("{getwd()}/"), template_name = "Question_2")

#Secondly I fetch the functions that I wrote from the code folder in Question 2
list.files('Question_2/code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
```

```{r, results='hide', message=FALSE, warning=FALSE}
Musictaste_df <- Inputting_Data(data_root = "Question_2/data/")
```

This function reads the csv files for Metallica and Coldplay and then combines them by binding their rows.

```{r, results='hide', message=FALSE, warning=FALSE}
Filtered_df <- Data_Filtering(Musictaste_df)
```

This functions alters the base dataframe to a more usable version. Firstly a variable is created that defines which band plays which songs. Then differently names album and duration variables are combined together. Then we filter out all songs that include "Live" or "Edit" in the title and remove all albums that have "Deluxe", "Live" or "Remaster" in the name, I also add back one original song that included "Live" in the title.

We select the variables we want and scale the relevant musical features to be on a scale between 0-100, except for tempo.

```{r, message=FALSE, warning=FALSE}
plot_box <- Plot_Pop_Box(Filtered_df)
plot_box
```

In the above function I firstly group by album and count the number of songs. This allows me to filter by albums that have more than _no_songs_ songs (where _no_songs_ can be set, default is 1). Secondly I then group by album and filter according to the albums that have more than _no_songs_ songs. I then plot a box plot for each album, ordering the x-axis according to release date. A jitter plot is added to show the individual songs, but with a low alpha and size to not detract from the box plot.

```{r, message=FALSE, warning=FALSE}
plot_hist <- Plot_Histogram(Filtered_df)
plot_hist
```

For this plot I first group by the band and summarise all numeric columns by mean. I then select the Band and all musical features and pivot the dataframe to a longer version with the musical features as rows and the bands as columns. I can then make a grouped bar plot to compare the musical features for the different bands. I use position = "dodge" to have spaces between the bar plots and stat = "identity" to use the values in the dataframe rather than the count() function.

```{r, message=FALSE, warning=FALSE}
Supp_df <- Preparing_Supplementary(data_root = "Question_2/data/")
```

This function read in and adapts the supplementary datasets of Spotify and Billboard to a combined functional dataset. Firstly Spotify is read in, duration is adapted and variables of interest are selected. 

Secondly, the Billboard rds file is read in and filtered to be songs that achieved a peak rank in a week of 20 or better. We then select variables of interest and group by song and artist. This allows us to arrange by date across the groups and keep the last week so that we have unique entries for each song on their last week in the chart.

Lastly we can join these dataframes together by song and artist, filter away the Billboard entries that do not have Spotify equals, we summarise across musical features and mutate the relevant features to again be on a scale from 0-100. We now have a dataset of the musical features of popular music.

```{r, message=FALSE, warning=FALSE}
Comparison_plot <- Comparing_Features(Filtered_df, Supp_df)
Comparison_plot
```

In this function we firstly rename the relevant variables in the first dataframe to appropiately leftjoin with the supplementary dataframe and pivot the table to have the musical features as columns. This allows me to plot a violin graph for each Band and popular music, which is then facet wrapped with the different musical features. Y scales are set to free and alpha is low for interpretability. 

##Question 3

```{r, results='hide', message=FALSE, warning=FALSE}
#Firstly Create the Texevier project
#Texevier::create_template_html(directory = glue::glue("{getwd()}/"), template_name = "Question_3")

#Secondly I fetch the functions that I wrote from the code folder in Question 3
list.files('Question_3/code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
```

The above code fetches the functions from the code folder under question 3.

```{r, results='hide', message=FALSE, warning=FALSE}
read_Data <- Data_Read(data_root = "Question_3/data/")
```

This function reads the 'Financial Allocations' and 'Financial Commitments' csv files and then joins them together according to country. 

```{r, results='hide', message=FALSE, warning=FALSE}
Map_df <- Make_Map_Data(read_Data)
```

In order to construct a geographical map of Europe we first create a dataframe using the ne_countries command from the rnaturalearth package that returns a dataset of polygons by country. The next step is to create a second dataframe which contains the data from our Data_Read() function, we then add a variable that calculates the commitment as a percentage of GDP.

To combine the dataframes we first mutate the variable containing country names to be called the same name and deal with one fringe case where the names for the same countries did not match between dataframes. We then merge the two dataframes by country names.

```{r, message=FALSE, warning=FALSE}
map_plot <- Create_Map(Map_df)
map_plot
```

The Create_Map() function firstly takes the polygons from the merged dataframe and links them to points on the map, these points are then changed into coordinates which are placed into the merged dataset. This data can then be used to plot a 'simple feature' graph that is filled according to the level of commitments. The limits of the coordinates are set to focus on Europe as the Australian news desk asked for a focus on Europe.

```{r, message=FALSE, warning=FALSE}
table <- Create_Table(read_Data, Markdown = TRUE)
table
```

The Create_Table() function takes the initial dataset and firstly removes missing values, then I calculate the total commitments and allocations for each country by adding their total individual commitments/allocations and their commitments/allocations made through the EU. Then I subtract the total allocations from the total commitments to calculate the difference in what was promised and what is given. 

I then arrange by the difference and filter the values so that only the extremes on either side shows up. The dataframe is then piped to a kable function which turns it into a table.

##Question 4

```{r, results='hide', message=FALSE, warning=FALSE}
#Firstly Create the Texevier project
#Texevier::create_template_html(directory = glue::glue("{getwd()}/"), template_name = "Question_4")


```

##Question 5

```{r, results='hide', message=FALSE, warning=FALSE}
#Firstly Create the Texevier project
#Texevier::create_template_html(directory = glue::glue("{getwd()}/"), template_name = "Question_5")

```


